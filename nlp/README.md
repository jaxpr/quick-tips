# ML6 NLP Quick Tips

Current content:

-  [_Multilingual Sentence Embeddings_ (21/01/2021)](2021_01_21_multilingual_sentence_embeddings):
Gives an overview of various current multilingual sentence embedding techniques and tools, and
how they compare given various sequence lengths.

-  [_Spacy 3.0_ (01/02/2021)](2021_02_01_spacy_3_projects):
Spacy 3.0 has just been released and in this tip, we'll have a look at some of the new features.
We'll be training a German NER model and streamline the end-to-end pipeline using the brand new spaCy projects!

-  [_Compact transformers_ (26/02/2021)](2021_02_26_compact_transformers):
Bigger isn't always better. In this tip we look at some compact BERT-based models that provide a nice balance
between computational resources and model accuracy.

-  [_Keyword Extraction with pke_ (18/03/2021)](2021_03_18_pke_keyword_extraction):
The KEYNG (read *king*) is dead, long live the KEYNG!
In this tip we look at `pke`, an alternative to Gensim for keyword extraction.

-  [_Explainable transformers using SHAP_ (22/04/2021)](2021_04_22_shap_for_huggingface_transformers):
BERT, explain yourself! üìñ
Up until recently language model predictions have lacked transparency. In this tip we look at `SHAP`, a way to explain your latest transformer based models.

-  [_Transformer-based Data Augmentation_ (18/06/2021)](2021_06_18_data_augmentation):
Ever struggled with having a limited non-English NLP dataset for a project? Fear not, data augmentation to the rescue ‚õëÔ∏è
In this week's tip, we look at backtranslation üîÄ and contextual word embedding insertions as data augmentation techniques for multilingual NLP. 

-  [_Neural Keyword Extraction_ (07/07/2021)](2021_07_07_neural_keyword_extraction):
Neural Keyword Extraction
In this week's tip, we look at neural keyword extraction methods and how they compare to classical methods.

-  [_Long range transformers_ (14/07/2021)](2021_06_29_long_range_transformers):
Beyond and above the 512! üèÖ In this week's tip, we look at novel long range transformer architectures and compare them against the well-known RoBERTa model.
