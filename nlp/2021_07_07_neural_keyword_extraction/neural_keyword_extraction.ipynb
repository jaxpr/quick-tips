{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Keyword Extraction Tip-of-the-week.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ak0IG15fsUVV"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne2ic1nmhs0n"
      },
      "source": [
        "# Neural Keyword Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j78lHqRwsUeU"
      },
      "source": [
        "In a [pervious quick tip](https://github.com/ml6team/quick-tips/tree/main/nlp/2021_03_18_pke_keyword_extraction) we looked at [`pke`](https://boudinfl.github.io/pke/build/html/index.html) as a replacement for Gensim's [recently removed keyword extraction module](https://github.com/RaRe-Technologies/gensim/releases/tag/4.0.0).\n",
        "`pke` comes with batteries included: It has preprocessing build in, supports non-English languages, and provides a wide range of keyword extraction methods: statistical, graph-based, and supervised.\n",
        "\n",
        "This makes `pke` a great choice to get started with keyword extraction, experiment with different methods and generate baselines to improve upon.\n",
        "\n",
        "But what if the required performance is not met by these *classical* methods?\n",
        "\n",
        "Newly-developed auspicious extraction methods fall into the category of **neural keyword extraction**.\n",
        "\n",
        "The methods often utilise **sequence-to-sequence models** based on recurrent neural networks (RNNs) or Long Short-Term Memory (LSTM). Their objective is to transform a sequence of input words (the given document) into an abstract intermediate representation and generate a sequence of keywords from it.\n",
        "\n",
        "These models do not use words or phrases directly, which enables them to generate unseen keywords as well. This is called **keyword generation** and combines abstractive as well as extractive keywords.\n",
        "\n",
        "They report **impressive performance** increases over the classical extraction methods.\n",
        "\n",
        "However, training such models\n",
        "**requires a large collection of documents** and annotated keywords, since the final training step is usually supervised.\n",
        "\n",
        "In addition, many of the models' repositories are not well maintained which makes it **difficult to train** them, especially on different languages or domains. \n",
        "\n",
        "This leaves the quesition of how neural keyword extraction can already be used today.\n",
        "\n",
        "Follow the below sections in this notebook to learn how to use **two approaches in the neural keyword extraction category** and how they compare to classical extraction methods. üëá"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak0IG15fsUVV"
      },
      "source": [
        "## üèó Getting started: Install packages & download models\n",
        "\n",
        "The below cells will setup everything that is required to get started with keyword extraction:\n",
        "\n",
        "* Install packages\n",
        "* Download additional resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbMg6Xyj8Xww",
        "outputId": "3b0b8599-77d5-41bf-b295-0ca49733b28e"
      },
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!pip install transformers\n",
        "!pip install keybert\n",
        "\n",
        "# Download additional resources\n",
        "!python -m nltk.downloader stopwords\n",
        "!python -m nltk.downloader universal_tagset\n",
        "!python -m spacy download en # Download English model"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-rlluee_4\n",
            "  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-rlluee_4\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (3.2.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (2.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (2.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.15.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (0.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 245kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (0.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.0.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->pke==1.8.1) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (57.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->pke==1.8.1) (0.22.2.post1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.7.4.3)\n",
            "Building wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-1.8.1-cp37-none-any.whl size=8763774 sha256=a2f5a1f79624196480a22f4466f5a24e96d990b19dc1a7f10a075174c570ef31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-io7dchf3/wheels/8d/24/54/6582e854e9e32dd6c632af6762b3a5d2f6b181c2992e165462\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-1.8.1 unidecode-1.2.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (0.22.2.post1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.7/dist-packages (from keybert) (2.0.0)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.7/dist-packages (from keybert) (10.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.8.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.10.0+cu102)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.9.0+cu102)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.0.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (0.4.4)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (3.7.4.3)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (0.9.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (4.5.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m‚úî Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEu54fBNsUPR"
      },
      "source": [
        "## KeyBERT\n",
        "\n",
        "Strictly speaking [KeyBERT](https://github.com/MaartenGr/KeyBERT) is not an end-to-end neural keyword extraction model. \n",
        "\n",
        "Nontheless the **underlying idea** is as clever as it is simple: It compares embeddings of words with embeddings of texts and selects the set of keywords which are most similar to the entire text.\n",
        "Both **word embeddings** as well was **text embeddings** are generated using **state-of-the-art neural models**, which have lead to tremendous performance improvements in other tasks.\n",
        "\n",
        "The benefit of this approach is that the used models are trained in an unsupervised manner and thus **do not require an annotated dataset** if keywords. In addition, they are available in a number of **non-English languages** as well.\n",
        "\n",
        "Let's see below how KeyBert can be used in practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV6d1GCs3sY6"
      },
      "source": [
        "from keybert import KeyBERT"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FdbT3Mi3xtY"
      },
      "source": [
        "kw_model = KeyBERT()\n",
        "\n",
        "def extract_keybert_keywords(text, keyphrase_ngram_range=(1,1), stop_words='english',\n",
        "                             use_maxsum=True, nr_candidates=20, top_n=5,\n",
        "                             use_mmr=False, diversity=0.7,\n",
        "                             only_keywords=True):\n",
        "    keyphrases = kw_model.extract_keywords(text, keyphrase_ngram_range=keyphrase_ngram_range, \n",
        "                                           stop_words=stop_words, use_maxsum=use_maxsum, \n",
        "                                           use_mmr=use_mmr, diversity=diversity, \n",
        "                                           nr_candidates=nr_candidates, top_n=top_n)\n",
        "    if only_keywords:\n",
        "        keyphrases = [phrase for phrase, score in keyphrases]\n",
        "    return keyphrases"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmYw5b02sUFq"
      },
      "source": [
        "## BART-based model\n",
        "\n",
        "ü§ó Hugging Face Model Hub is *the* prime address when it comes to discovering state-of-the-art models that are easy to use. However, with respect to neural keyword (or keyphrase) extraction there are only two models available as of this writing.\n",
        "\n",
        "Both models were created by [Ankur Singh](https://huggingface.co/ankur310794) and use a **BART-based sequence-to-sequence architecture**. Unfortunately is not a lot of details available about the training specific \n",
        "\n",
        "[One model](https://huggingface.co/ankur310794/bart-base-keyphrase-generation-kpTimes) was trained using the [KPTimes dataset](https://aclanthology.org/W19-8617/), a large dataset consisting of **English news articles** and hand-annotated keywords.\n",
        "\n",
        "[The other](https://huggingface.co/ankur310794/bart-base-keyphrase-generation-openkp) was trained using the [OpenKP dataset](https://github.com/microsoft/OpenKP), which contains a large number of **English web documents** and up to three most relevant keywords. From this This restriction holds true for the model as well: it will return at most three keywords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnexjK78yvYr"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "  \n",
        "huggingface_models = {\n",
        "    # Trained on OpenKP: Returns up to 3 keywords\n",
        "    'openkp': \"ankur310794/bart-base-keyphrase-generation-openkp\",\n",
        "    # Trained on KPTimes\n",
        "    'kptimes': \"ankur310794/bart-base-keyphrase-generation-kpTimes\",\n",
        "}\n",
        "\n",
        "def load_model(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model(huggingface_models['kptimes'])\n",
        "\n",
        "def extract_keywords_using_bart(text):\n",
        "    encoded_text = tokenizer.prepare_seq2seq_batch(\n",
        "        [text],\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    encoded_keywords = model.generate(**encoded_text)\n",
        "    raw_keywords = tokenizer.batch_decode(\n",
        "        encoded_keywords, \n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    keywords = [keyword.strip() for keyword_string in raw_keywords\n",
        "                                for keyword in keyword_string.split(';')]\n",
        "    return keywords"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaCNrz678qXF"
      },
      "source": [
        "##‚öîÔ∏è Classical vs. Neural Keyword Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ertAe4GXCal5"
      },
      "source": [
        "### Classical extraction methods\n",
        "\n",
        "The code below wraps several extraction methods from pke into convenience functions that use the default parameters and only require an (English) text from which keywords will be extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xhjI7xo8pK5"
      },
      "source": [
        "import string\n",
        "from itertools import zip_longest\n",
        "\n",
        "import pke\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Convenience functions for pke keyword extraction\n",
        "\n",
        "## Statistical models\n",
        "def extract_tfidf_keywords(text, top_n=10, language='en', normalization=None, \n",
        "                           n_grams=3, only_keywords=True):\n",
        "    stoplist = list(string.punctuation)\n",
        "    stoplist += stopwords.words('english')\n",
        "    extractor = pke.unsupervised.TfIdf()\n",
        "    extractor.load_document(input=text, language=language, normalization=normalization)\n",
        "    extractor.candidate_selection(n=n_grams, stoplist=stoplist)\n",
        "    extractor.candidate_weighting()\n",
        "    keyphrases = extractor.get_n_best(n=top_n)\n",
        "    if only_keywords:\n",
        "        keyphrases = [phrase for phrase, score in keyphrases]\n",
        "    return keyphrases\n",
        "\n",
        "def extract_yake_keywords(text, top_n=10, normalization=None, window=2, \n",
        "                          threshold=0.8, language='en', n=3, use_stems=False, \n",
        "                          only_keywords=True):\n",
        "    stoplist = stopwords.words('english')\n",
        "    extractor = pke.unsupervised.YAKE()\n",
        "    extractor.load_document(input=text, language=language, normalization=normalization)\n",
        "    extractor.candidate_selection(n=n, stoplist=stoplist)\n",
        "    extractor.candidate_weighting(window=window, stoplist=stoplist, use_stems=use_stems)\n",
        "    keyphrases = extractor.get_n_best(n=top_n, threshold=threshold)\n",
        "    if only_keywords:\n",
        "        keyphrases = [phrase for phrase, score in keyphrases]\n",
        "    return keyphrases\n",
        "\n",
        "## Graph-based algorithms\n",
        "def extract_textrank_keywords(text, top_n=10, language='en', normalization=None, \n",
        "                              window=2, top_percent=0.33, only_keywords=True):\n",
        "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
        "    extractor = pke.unsupervised.TextRank()\n",
        "    extractor.load_document(input=text, language=language, normalization=normalization)\n",
        "    extractor.candidate_weighting(window=window, pos=pos, top_percent=top_percent)\n",
        "    keyphrases = extractor.get_n_best(n=top_n)\n",
        "    if only_keywords:\n",
        "        keyphrases = [phrase for phrase, score in keyphrases]\n",
        "    return keyphrases\n",
        "\n",
        "def extract_topicrank_keywords(text, top_n=10, language='en', only_keywords=True):\n",
        "    extractor = pke.unsupervised.TopicRank()\n",
        "    extractor.load_document(input=text, language=language)\n",
        "    extractor.candidate_selection()\n",
        "    extractor.candidate_weighting()\n",
        "    keyphrases = extractor.get_n_best(n=top_n)\n",
        "    if only_keywords:\n",
        "        keyphrases = [phrase for phrase, score in keyphrases]\n",
        "    return keyphrases"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI8OguDTcYL1"
      },
      "source": [
        "The next cell:\n",
        "\n",
        "* **collects the above functions** for keyword extraction together with a set of keyword arguments for easy access in a dictionary,\n",
        "* sets a **default subset of extraction functions** to compare, and \n",
        "* defines a **convenience function** that simplifies the **comparison** of the different extraction methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGa7LqW2-Omy"
      },
      "source": [
        "# Define extraction functions, labels, and set parameters\n",
        "top_n = 10\n",
        "\n",
        "KEYWORD_EXTRACTION_FUNCTIONS = {\n",
        "    # Neural Keyword Extraction \n",
        "    'KeyBERT': (\n",
        "        extract_keybert_keywords, \n",
        "        {\n",
        "            'keyphrase_ngram_range': (1,2),\n",
        "            'stop_words': 'english',\n",
        "            'use_maxsum': True, \n",
        "            'nr_candidates': 20, \n",
        "            'top_n': 10, \n",
        "            'use_mmr': False, \n",
        "            'diversity': 0.7,\n",
        "        },\n",
        "    ),\n",
        "    'BART-based': (\n",
        "        extract_keywords_using_bart, \n",
        "        {},\n",
        "    ),\n",
        "\n",
        "    # Statistical models\n",
        "    'TFIDF': (\n",
        "        extract_tfidf_keywords, \n",
        "        {'top_n': top_n},\n",
        "    ),\n",
        "    'YAKE': (\n",
        "        extract_yake_keywords, \n",
        "        {'top_n': top_n},\n",
        "    ),\n",
        "    # Graph-based models\n",
        "    'TextRank': (\n",
        "        extract_textrank_keywords, \n",
        "        {'top_n': top_n ,'window': 2},\n",
        "    ),\n",
        "    'TopicRank': (\n",
        "        extract_topicrank_keywords, \n",
        "        {'top_n': top_n},\n",
        "    ),\n",
        "}\n",
        "\n",
        "DEFAULT_SELECTION = ['KeyBERT', 'BART-based', 'TFIDF', 'YAKE', 'TextRank', 'TopicRank']\n",
        "\n",
        "def compare_keyword_extraction_algorithms(text, \n",
        "                                          keyword_extraction_functions=None,\n",
        "                                          selection=None):\n",
        "    \"\"\"Convenience function compare extracted keywords from the given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to extract keywords from.\n",
        "        keyword_extraction_functions (dict): Dict contaning labels as keys and\n",
        "            a tuple of (extraction_function, kwargs) as values. Defaults to None.\n",
        "        selection (list): List of names of algorithm to use for keyword \n",
        "            extraction. See keyword_extraction_functions for possible values\n",
        "            and/or to change arguments. Defaults to None.\n",
        "    \"\"\"\n",
        "    if keyword_extraction_functions is None:\n",
        "        keyword_extraction_functions = KEYWORD_EXTRACTION_FUNCTIONS\n",
        "    if selection is None:\n",
        "        selection = DEFAULT_SELECTION\n",
        "    \n",
        "    # Create DataFrame with extracted keywords\n",
        "    all_keywords = pd.DataFrame(\n",
        "        zip_longest(\n",
        "            *(extraction_fn(text, **kwargs)\n",
        "                for name, (extraction_fn, kwargs) in keyword_extraction_functions.items()\n",
        "                if name in selection\n",
        "            ),\n",
        "            fillvalue=\"\",\n",
        "        ),\n",
        "        columns=selection,\n",
        "    )\n",
        "    \n",
        "    # Display table\n",
        "    display(all_keywords)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c12HR6HKWol"
      },
      "source": [
        "### Extracted Keywords\n",
        "\n",
        "With the keyword extractions functions implemented let's define a **few short example texts** which will be used below for keyword extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAs0XolbEQNh"
      },
      "source": [
        "texts = [\n",
        "    # Dartmouth Workshop\n",
        "    # https://en.wikipedia.org/wiki/Dartmouth_workshop\n",
        "    (\n",
        "        \"The Dartmouth Summer Research Project on Artificial Intelligence was \"\n",
        "        \"a 1956 summer workshop widely considered to be the founding event of \"\n",
        "        \"artificial intelligence as a field. The project lasted approximately \"\n",
        "        \"six to eight weeks and was essentially an extended brainstorming \"\n",
        "        \"session. Eleven mathematicians and scientists originally planned to \"\n",
        "        \"attend; not all of them attended, but more than ten others came for \"\n",
        "        \"short times.\"\n",
        "    ),\n",
        "    # Abstract TextRank Paper\n",
        "    (\n",
        "        \"In this paper, we introduce TextRank ‚Äì a graph-based ranking model \" \n",
        "        \"for text processing, and show how this model can be successfully \"\n",
        "        \"used in natural language applications. In particular, we propose \"\n",
        "        \"two innovative unsupervised methods for keyword and sentence \"\n",
        "        \"extraction, and show that the results obtained compare favorably \"\n",
        "        \"with previously published results on established benchmarks.\"\n",
        "     ),\n",
        "    # News\n",
        "    # https://www.nytimes.com/live/2021/02/09/us/trump-impeachment-trial\n",
        "    (\n",
        "        \"The House managers prosecuting former President Donald J. Trump \"\n",
        "        \"opened his Senate impeachment trial on Tuesday with a vivid and \"\n",
        "        \"graphic sequence of footage of his supporters storming the Capitol \"\n",
        "        \"last month in an effort to prevent Congress from finalizing his \"\n",
        "        \"election defeat.\\n\"\n",
        "        \"The managers wasted no time moving immediately to their most powerful \"\n",
        "        \"evidence: the explicit visual record of the deadly Capitol siege \"\n",
        "        \"that threatened the lives of former Vice President Mike Pence and \"\n",
        "        \"members of both houses of Congress juxtaposed against Mr. Trump‚Äôs \"\n",
        "        \"own words encouraging members of the mob at a rally beforehand.\\n\"\n",
        "        \"The scenes of mayhem and violence ‚Äî punctuated by expletives rarely \"\n",
        "        \"heard on the floor of the Senate ‚Äî highlighted the drama of the \"\n",
        "        \"trial in gut-punching fashion for the senators who lived through \"\n",
        "        \"the events barely a month ago and now sit as quasi-jurors. On the \"\n",
        "        \"screens, they saw enraged extremists storming barricades, beating \"\n",
        "        \"police officers, setting up a gallows and yelling, ‚ÄúTake the \"\n",
        "        \"building,‚Äù ‚ÄúFight for Trump‚Äù and ‚ÄúPence is a traitor! Traitor Pence!‚Äù\"\n",
        "    ),\n",
        "    # Recipe\n",
        "    # https://www.nytimes.com/2021/02/08/dining/birria-recipes.html\n",
        "    (\n",
        "        \"You go to Birrieria Nochistl√°n for the Moreno family‚Äôs \"\n",
        "        \"Zacatecan-style birria ‚Äî a big bowl of hot goat meat submerged \"\n",
        "        \"in a dark pool of its own concentrated cooking juices.\\n\"\n",
        "        \"Right out of the pot, the steamed meat isn‚Äôt just tender, but \"\n",
        "        \"in places deliciously sticky, smudged with chile adobo, falling \"\n",
        "        \"apart, barely even connected to the bone. It comes with thick, \"\n",
        "        \"soft tortillas, made to order, and a vibrant salsa roja. \"\n",
        "        \"The Moreno family has been serving birria exactly like this for \"\n",
        "        \"about 20 years.\\n\"\n",
        "        \"‚ÄúSometimes I think we should update our menu,‚Äù said Rosio Moreno, \"\n",
        "        \"23, whose parents started the business out of their home in East \"\n",
        "        \"Los Angeles. ‚ÄúBut we don‚Äôt want to change the way we do things \"\n",
        "        \"because of the hype.‚Äù\"\n",
        "    ),\n",
        "    # The text within this notebook:\n",
        "    (\n",
        "        \"In a pervious quick tip we looked at pke as a replacement for Gensim's \"\n",
        "        \"recently removed keyword extraction module. pke comes with batteries \"\n",
        "        \"included: It has preprocessing build in, supports non-English languages, \"\n",
        "        \"and provides a wide range of keyword extraction methods: statistical, \"\n",
        "        \"graph-based, and supervised.\\n\" \n",
        "        \"This makes pke a great choice to get started with keyword extraction, \"\n",
        "        \"experiment with different methods and generate baselines to improve upon.\\n\"\n",
        "        \"But what if the required performance is not met by these classical methods?\\n\"\n",
        "        \"Newly-developed auspicious extraction methods fall into the category of \"\n",
        "        \"neural keyword extraction.\\n\"\n",
        "        \"The methods often utilise sequence-to-sequence models based on recurrent \"\n",
        "        \"neural networks (RNNs) or Long Short-Term Memory (LSTM). Their objective \"\n",
        "        \"is to transform a sequence of input words (the given document) into an \"\n",
        "        \"abstract intermediate representation and generate a sequence of keywords \"\n",
        "        \"from it.\\n\"\n",
        "        \"These models do not use words or phrases directly, which enables them to \"\n",
        "        \"generate unseen keywords as well. This is called keyword generation and \"\n",
        "        \"combines abstractive as well as extractive keywords.\\n\"\n",
        "        \"They report impressive performance increases over the classical \"\n",
        "        \"extraction methods.\\n\"\n",
        "        \"However, training such models requires a large collection of documents \"\n",
        "        \"and annotated keywords, since the final training step is usually \"\n",
        "        \"supervised.\\n\"\n",
        "        \"In addition, many of the models' repositories are not well maintained \"\n",
        "        \"which makes it difficult to train them, especially on different \"\n",
        "        \"languages or domains.\\n\"\n",
        "        \"This leaves the quesition of how neural keyword extraction can already \"\n",
        "        \"be used today.\\n\"\n",
        "        \"Follow the below sections in this notebook to learn how to use two \"\n",
        "        \"approaches in the neural keyword extraction category and how they \"\n",
        "        \"compare to classical extraction methods.\"\n",
        "    ),\n",
        "]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RFgnm9mWEQNj",
        "outputId": "90473f59-16f4-4bb6-9de9-21c7a817a358"
      },
      "source": [
        "# Compare the keywords extracted by the given algorithms\n",
        "selected_algorithms = ['KeyBERT', 'BART-based', 'TFIDF','YAKE', 'TextRank', 'TopicRank']\n",
        "\n",
        "for text in texts:\n",
        "    compare_keyword_extraction_algorithms(text, selection=selected_algorithms)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
            "WARNING:root:Candidates are generated using 0.33-top\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KeyBERT</th>\n",
              "      <th>BART-based</th>\n",
              "      <th>TFIDF</th>\n",
              "      <th>YAKE</th>\n",
              "      <th>TextRank</th>\n",
              "      <th>TopicRank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>field project</td>\n",
              "      <td>Dartmouth University</td>\n",
              "      <td>artificial</td>\n",
              "      <td>dartmouth summer research</td>\n",
              "      <td>summer research</td>\n",
              "      <td>artificial intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>artificial</td>\n",
              "      <td>Artificial intelligence</td>\n",
              "      <td>artificial intelligence</td>\n",
              "      <td>summer research project</td>\n",
              "      <td>artificial intelligence</td>\n",
              "      <td>mathematicians</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>originally planned</td>\n",
              "      <td></td>\n",
              "      <td>intelligence</td>\n",
              "      <td>1956 summer workshop</td>\n",
              "      <td>summer</td>\n",
              "      <td>field</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>intelligence</td>\n",
              "      <td></td>\n",
              "      <td>summer</td>\n",
              "      <td>summer workshop widely</td>\n",
              "      <td>brainstorming</td>\n",
              "      <td>event</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>summer research</td>\n",
              "      <td></td>\n",
              "      <td>dartmouth summer</td>\n",
              "      <td>artificial intelligence</td>\n",
              "      <td>short</td>\n",
              "      <td>extended brainstorming session</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>scientists originally</td>\n",
              "      <td></td>\n",
              "      <td>dartmouth summer research</td>\n",
              "      <td>workshop widely considered</td>\n",
              "      <td></td>\n",
              "      <td>project</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>dartmouth</td>\n",
              "      <td></td>\n",
              "      <td>summer research</td>\n",
              "      <td>dartmouth summer</td>\n",
              "      <td></td>\n",
              "      <td>scientists</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>brainstorming session</td>\n",
              "      <td></td>\n",
              "      <td>summer research project</td>\n",
              "      <td>summer research</td>\n",
              "      <td></td>\n",
              "      <td>weeks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>project lasted</td>\n",
              "      <td></td>\n",
              "      <td>1956</td>\n",
              "      <td>research project</td>\n",
              "      <td></td>\n",
              "      <td>dartmouth summer research project</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>dartmouth summer</td>\n",
              "      <td></td>\n",
              "      <td>1956 summer</td>\n",
              "      <td>1956 summer</td>\n",
              "      <td></td>\n",
              "      <td>summer workshop</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 KeyBERT  ...                          TopicRank\n",
              "0          field project  ...            artificial intelligence\n",
              "1             artificial  ...                     mathematicians\n",
              "2     originally planned  ...                              field\n",
              "3           intelligence  ...                              event\n",
              "4        summer research  ...     extended brainstorming session\n",
              "5  scientists originally  ...                            project\n",
              "6              dartmouth  ...                         scientists\n",
              "7  brainstorming session  ...                              weeks\n",
              "8         project lasted  ...  dartmouth summer research project\n",
              "9       dartmouth summer  ...                    summer workshop\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
            "WARNING:root:Candidates are generated using 0.33-top\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KeyBERT</th>\n",
              "      <th>BART-based</th>\n",
              "      <th>TFIDF</th>\n",
              "      <th>YAKE</th>\n",
              "      <th>TextRank</th>\n",
              "      <th>TopicRank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>unsupervised methods</td>\n",
              "      <td>Text</td>\n",
              "      <td>results</td>\n",
              "      <td>natural language applications</td>\n",
              "      <td>sentence extraction</td>\n",
              "      <td>model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>processing model</td>\n",
              "      <td>Computers and the Internet</td>\n",
              "      <td>introduce</td>\n",
              "      <td>based ranking model</td>\n",
              "      <td>text processing</td>\n",
              "      <td>results</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ranking</td>\n",
              "      <td></td>\n",
              "      <td>introduce textrank</td>\n",
              "      <td>introduce textrank</td>\n",
              "      <td>unsupervised</td>\n",
              "      <td>keyword</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>graph based</td>\n",
              "      <td></td>\n",
              "      <td>textrank</td>\n",
              "      <td>based ranking</td>\n",
              "      <td>language</td>\n",
              "      <td>innovative unsupervised methods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>language applications</td>\n",
              "      <td></td>\n",
              "      <td>based</td>\n",
              "      <td>text processing</td>\n",
              "      <td></td>\n",
              "      <td>sentence extraction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>text</td>\n",
              "      <td></td>\n",
              "      <td>based ranking</td>\n",
              "      <td>language applications</td>\n",
              "      <td></td>\n",
              "      <td>text processing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>paper introduce</td>\n",
              "      <td></td>\n",
              "      <td>based ranking model</td>\n",
              "      <td>successfully used</td>\n",
              "      <td></td>\n",
              "      <td>graph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>sentence extraction</td>\n",
              "      <td></td>\n",
              "      <td>ranking</td>\n",
              "      <td>natural language</td>\n",
              "      <td></td>\n",
              "      <td>natural language applications</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>extraction results</td>\n",
              "      <td></td>\n",
              "      <td>ranking model</td>\n",
              "      <td>ranking model</td>\n",
              "      <td></td>\n",
              "      <td>particular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>based ranking</td>\n",
              "      <td></td>\n",
              "      <td>text processing</td>\n",
              "      <td>textrank</td>\n",
              "      <td></td>\n",
              "      <td>textrank</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 KeyBERT  ...                        TopicRank\n",
              "0   unsupervised methods  ...                            model\n",
              "1       processing model  ...                          results\n",
              "2                ranking  ...                          keyword\n",
              "3            graph based  ...  innovative unsupervised methods\n",
              "4  language applications  ...              sentence extraction\n",
              "5                   text  ...                  text processing\n",
              "6        paper introduce  ...                            graph\n",
              "7    sentence extraction  ...    natural language applications\n",
              "8     extraction results  ...                       particular\n",
              "9          based ranking  ...                         textrank\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
            "WARNING:root:Candidates are generated using 0.33-top\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KeyBERT</th>\n",
              "      <th>BART-based</th>\n",
              "      <th>TFIDF</th>\n",
              "      <th>YAKE</th>\n",
              "      <th>TextRank</th>\n",
              "      <th>TopicRank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>trump</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>pence</td>\n",
              "      <td>former president donald</td>\n",
              "      <td>capitol last</td>\n",
              "      <td>senate impeachment trial</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mr trump</td>\n",
              "      <td>US Politics</td>\n",
              "      <td>trump</td>\n",
              "      <td>house managers prosecuting</td>\n",
              "      <td>j. trump</td>\n",
              "      <td>members</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>house managers</td>\n",
              "      <td>Impeachment</td>\n",
              "      <td>managers</td>\n",
              "      <td>prosecuting former president</td>\n",
              "      <td>enraged extremists</td>\n",
              "      <td>congress</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fight trump</td>\n",
              "      <td>House of Representatives</td>\n",
              "      <td>president</td>\n",
              "      <td>former vice president</td>\n",
              "      <td>own words</td>\n",
              "      <td>capitol last month</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mike pence</td>\n",
              "      <td>Senate</td>\n",
              "      <td>senate</td>\n",
              "      <td>capitol last month</td>\n",
              "      <td>powerful evidence</td>\n",
              "      <td>house managers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>prosecuting</td>\n",
              "      <td>Congress</td>\n",
              "      <td>storming</td>\n",
              "      <td>vice president mike</td>\n",
              "      <td>election defeat</td>\n",
              "      <td>pence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>senate highlighted</td>\n",
              "      <td></td>\n",
              "      <td>capitol</td>\n",
              "      <td>president mike pence</td>\n",
              "      <td>graphic sequence</td>\n",
              "      <td>former vice president mike pence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>opened senate</td>\n",
              "      <td></td>\n",
              "      <td>members</td>\n",
              "      <td>managers prosecuting former</td>\n",
              "      <td>house managers</td>\n",
              "      <td>mayhem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>managers prosecuting</td>\n",
              "      <td></td>\n",
              "      <td>congress</td>\n",
              "      <td>senate impeachment trial</td>\n",
              "      <td>capitol</td>\n",
              "      <td>graphic sequence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>impeachment trial</td>\n",
              "      <td></td>\n",
              "      <td>traitor</td>\n",
              "      <td>president donald</td>\n",
              "      <td>president</td>\n",
              "      <td>trial</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                KeyBERT  ...                         TopicRank\n",
              "0                 trump  ...          senate impeachment trial\n",
              "1              mr trump  ...                           members\n",
              "2        house managers  ...                          congress\n",
              "3           fight trump  ...                capitol last month\n",
              "4            mike pence  ...                    house managers\n",
              "5           prosecuting  ...                             pence\n",
              "6    senate highlighted  ...  former vice president mike pence\n",
              "7         opened senate  ...                            mayhem\n",
              "8  managers prosecuting  ...                  graphic sequence\n",
              "9     impeachment trial  ...                             trial\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
            "WARNING:root:Candidates are generated using 0.33-top\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KeyBERT</th>\n",
              "      <th>BART-based</th>\n",
              "      <th>TFIDF</th>\n",
              "      <th>YAKE</th>\n",
              "      <th>TextRank</th>\n",
              "      <th>TopicRank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hot goat</td>\n",
              "      <td>Restaurant</td>\n",
              "      <td>moreno</td>\n",
              "      <td>concentrated cooking juices</td>\n",
              "      <td>concentrated cooking</td>\n",
              "      <td>moreno family</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>deliciously sticky</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>moreno family</td>\n",
              "      <td>birrieria nochistl√°n</td>\n",
              "      <td>goat meat</td>\n",
              "      <td>style birria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>goat meat</td>\n",
              "      <td></td>\n",
              "      <td>family</td>\n",
              "      <td>hot goat meat</td>\n",
              "      <td>big bowl</td>\n",
              "      <td>zacatecan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>places deliciously</td>\n",
              "      <td></td>\n",
              "      <td>birria</td>\n",
              "      <td>goat meat submerged</td>\n",
              "      <td>style birria</td>\n",
              "      <td>places</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nochistl√°n moreno</td>\n",
              "      <td></td>\n",
              "      <td>meat</td>\n",
              "      <td>cooking juices</td>\n",
              "      <td>birrieria nochistl√°n</td>\n",
              "      <td>sticky</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>meat isn</td>\n",
              "      <td></td>\n",
              "      <td>birrieria</td>\n",
              "      <td>big bowl</td>\n",
              "      <td>los</td>\n",
              "      <td>big bowl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>rosio moreno</td>\n",
              "      <td></td>\n",
              "      <td>birrieria nochistl√°n</td>\n",
              "      <td>hot goat</td>\n",
              "      <td>salsa</td>\n",
              "      <td>thick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>vibrant salsa</td>\n",
              "      <td></td>\n",
              "      <td>nochistl√°n</td>\n",
              "      <td>dark pool</td>\n",
              "      <td>moreno</td>\n",
              "      <td>hot goat meat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>birrieria</td>\n",
              "      <td></td>\n",
              "      <td>zacatecan</td>\n",
              "      <td>concentrated cooking</td>\n",
              "      <td>meat</td>\n",
              "      <td>home</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>birria exactly</td>\n",
              "      <td></td>\n",
              "      <td>style birria</td>\n",
              "      <td>goat meat</td>\n",
              "      <td>birria</td>\n",
              "      <td>soft tortillas</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              KeyBERT   BART-based  ...              TextRank       TopicRank\n",
              "0            hot goat   Restaurant  ...  concentrated cooking   moreno family\n",
              "1  deliciously sticky  Los Angeles  ...             goat meat    style birria\n",
              "2           goat meat               ...              big bowl       zacatecan\n",
              "3  places deliciously               ...          style birria          places\n",
              "4   nochistl√°n moreno               ...  birrieria nochistl√°n          sticky\n",
              "5            meat isn               ...                   los        big bowl\n",
              "6        rosio moreno               ...                 salsa           thick\n",
              "7       vibrant salsa               ...                moreno   hot goat meat\n",
              "8           birrieria               ...                  meat            home\n",
              "9      birria exactly               ...                birria  soft tortillas\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
            "WARNING:root:Candidates are generated using 0.33-top\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KeyBERT</th>\n",
              "      <th>BART-based</th>\n",
              "      <th>TFIDF</th>\n",
              "      <th>YAKE</th>\n",
              "      <th>TextRank</th>\n",
              "      <th>TopicRank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>methods unable</td>\n",
              "      <td>Search Engines</td>\n",
              "      <td>pke</td>\n",
              "      <td>pervious quick tip</td>\n",
              "      <td>extraction methods</td>\n",
              "      <td>keyword extraction module</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>keyword extraction</td>\n",
              "      <td>Gensim</td>\n",
              "      <td>keyword extraction</td>\n",
              "      <td>recently removed keyword</td>\n",
              "      <td>- english</td>\n",
              "      <td>pke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>languages provdes</td>\n",
              "      <td></td>\n",
              "      <td>extraction</td>\n",
              "      <td>keyword extraction module</td>\n",
              "      <td>wide range</td>\n",
              "      <td>different methods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pervious quick</td>\n",
              "      <td></td>\n",
              "      <td>methods</td>\n",
              "      <td>recently removed</td>\n",
              "      <td>extraction</td>\n",
              "      <td>experiment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>improve methods</td>\n",
              "      <td></td>\n",
              "      <td>keyword</td>\n",
              "      <td>removed keyword extraction</td>\n",
              "      <td>quick</td>\n",
              "      <td>statistical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>replacement gensim</td>\n",
              "      <td></td>\n",
              "      <td>pervious</td>\n",
              "      <td>pervious quick</td>\n",
              "      <td>-</td>\n",
              "      <td>replacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>methods generate</td>\n",
              "      <td></td>\n",
              "      <td>pervious quick</td>\n",
              "      <td>quick tip</td>\n",
              "      <td>methods</td>\n",
              "      <td>gensim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>extraction module</td>\n",
              "      <td></td>\n",
              "      <td>pervious quick tip</td>\n",
              "      <td>keyword extraction</td>\n",
              "      <td>great</td>\n",
              "      <td>graph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>pke comes</td>\n",
              "      <td></td>\n",
              "      <td>quick tip</td>\n",
              "      <td>extraction module</td>\n",
              "      <td></td>\n",
              "      <td>great choice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>pke replacement</td>\n",
              "      <td></td>\n",
              "      <td>tip</td>\n",
              "      <td>gensim</td>\n",
              "      <td></td>\n",
              "      <td>wide range</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              KeyBERT  ...                  TopicRank\n",
              "0      methods unable  ...  keyword extraction module\n",
              "1  keyword extraction  ...                        pke\n",
              "2   languages provdes  ...          different methods\n",
              "3      pervious quick  ...                 experiment\n",
              "4     improve methods  ...                statistical\n",
              "5  replacement gensim  ...                replacement\n",
              "6    methods generate  ...                     gensim\n",
              "7   extraction module  ...                      graph\n",
              "8           pke comes  ...               great choice\n",
              "9     pke replacement  ...                 wide range\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev2r_xMKCPJ5"
      },
      "source": [
        "## üßë‚Äçüî¨ Try it yourself!\n",
        "\n",
        "**Task**: \n",
        "\n",
        "1. Insert your own text that you would like to extract keywords from\n",
        "2. Select the desired keyword extraction methods\n",
        "3. Extract keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHbZr2a8CPJ7"
      },
      "source": [
        "# Task 1: Add your own input text to e\n",
        "text = \"Replace this string rambling on about keyword extraction and how great it is with your own text\"\n",
        "\n",
        "# Task 2: Select the desired keyword extraction methods you want to compare\n",
        "selected_algorithms = ['KeyBERT', 'BART-based', 'TFIDF', 'YAKE', 'TextRank', 'TopicRank']\n",
        "\n",
        "# Task 3: Execute this cell to compare the extracted keywords\n",
        "compare_keyword_extraction_algorithms(text, selection=selected_algorithms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wfoRIn1zPQD"
      },
      "source": [
        "To-do: \n",
        "\n",
        "* [ ] Update summary\n",
        "    * Requires data\n",
        "    * Requires training from scratch\n",
        "    * Repositories are not frequently updated\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook gave a brief overview of neural keyword extraction.\n",
        "\n",
        "If an annotated dataset is available or if the problem domain is closely related to one of the existing dataset and it's possible to invest some time into training a model yourself, neural keyword extraction **promises great performance**.\n",
        "\n",
        "We further presented two approaches that are readily available: **KeyBERT** uses the **similarity** between word and text **embeddings** to find keywords which best describe a text and an **end-to-end neural keyword extraction model** based on BART.\n",
        "\n",
        "For now, we recommend to first try `pke` to establish a solid baseline and explore the mentioned extraction methods as an addition.\n",
        "\n",
        "\n",
        "## Resources\n",
        "\n",
        "### üìö Libraries & Packages\n",
        "\n",
        "* [**KeyBERT**](https://github.com/MaartenGr/KeyBERT): Keyword extraction method, that chooses keywords whose word embeddings are most similar to embeddings of the entire text\n",
        "* **BART-based neural keyword extraction** models [trained on the KPTimes dataset](https://huggingface.co/ankur310794/bart-base-keyphrase-generation-kpTimes) or [OpenKP dataset](https://huggingface.co/ankur310794/bart-base-keyphrase-generation-openkp) on ü§ó Model Hub.\n",
        "* [**`pke`** python keyphrase extraction](https://github.com/boudinfl/pke): Neat library implementing amongst others TF-IDF, YAKE, KPMiner, TextRank, SingleRank, TopicRank, TopologicalPageRank, PositionRank, MultipartiteRank, KEA, and WINGNUS. Uses GPLv3 licence.[[documentation](https://boudinfl.github.io/pke/)]\n",
        "* [**YAKE**](https://github.com/LIAAD/yake): An alternative implementation from the authors of the YAKE paper.\n",
        "\n",
        "\n",
        "### üìÑ Overview Papers\n",
        "\n",
        "* *Keyword extraction: a review of methods and approaches* by Slobodan Beliga (2014)\n",
        " [[paper](http://langnet.uniri.hr/papers/beliga/Beliga_KeywordExtraction_a_review_of_methods_and_approaches.pdf)]\n",
        "* *A Review of Keyphrase Extraction* by Eirini Papagiannopoulou and Grigorios Tsoumakas (2019) [[paper](https://arxiv.org/pdf/1905.05044)]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr1oYx4_tMrp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}