# Data augmentation

Ever struggled with having a limited non-English NLP dataset for a project? ðŸ¤¯ Fear not, data augmentation to the rescue â›‘
In this week's tip, we look at backtranslation ðŸ”€ and contextual word embedding insertions as a data augmentation techniques for multilingual NLP. We'll be using the MariaMT and distilled BERT pre-trained models, available on huggingface. 

We recommend to open the notebook using Colab to for an interactive explainable experience and optimal rendering of the visuals ðŸ‘‡:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ml6team/quick-tips/blob/main/nlp/2021_06_18_data_augmentation/totw_nlp_dat_aug.ipynb)
