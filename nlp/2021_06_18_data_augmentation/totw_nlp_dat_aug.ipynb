{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of totw_nlp_dat_aug.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXro9efVU84k"
      },
      "source": [
        "# üìà Transformer-based Data Augmentation in NLP \n",
        "\n",
        "The training size will impact the performace of a model heavily, this notebook looks into the possibilities of performing data augmentation on a NLP dataset. Data augmentation techniques are used to generate additional samples ü™Ñ. \n",
        "\n",
        "Data augmentation is already standard practice in computer vision projects üëå, but can also be leveraged in multilingual NLP problems. We'll use a limited trainingset to simulate a real-world use case, where we often are constrained by the size of the available data ü§¶. \n",
        "\n",
        "We'll focuss on using back-translation and contextual word-embedding insertions as data augmentation techniques ü§ó."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acwqGIAyU84m"
      },
      "source": [
        "## üõ†Ô∏è Getting started\n",
        "\n",
        "The cells below will setup everything that is required to get started with data augmentation and finetuning an NLP model with the HuggingFace API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buIDjl_wU84m"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-ItvjqP4Cxn"
      },
      "source": [
        "!pip install -q transformers sentencepiece datasets tokenizers nltk nlpaug "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir6zD__FU84n"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM-09Chsj7wI"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "import nltk\n",
        "import nlpaug.flow as naf\n",
        "import nlpaug.augmenter.word as naw\n",
        "import plotly.graph_objects as go\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TrainerCallback\n",
        "from datasets import load_dataset, concatenate_datasets, load_from_disk, load_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFQ6LEgjeRWb"
      },
      "source": [
        "### Download dataset\n",
        "Since we're particulary interested in multilingual NLP, we'll use a well-known dutch dataset [DBRD](https://github.com/benjaminvdb/DBRD). The dataset contains over 110k book reviews along with associated binary sentiment polarity labels. The downstream task will be assigning a sentiment to a book review.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ltsurmvaaGd"
      },
      "source": [
        "max_input_len=128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "book_review_ds = load_dataset(\"dbrd\").filter(lambda e: len(tokenizer.batch_encode_plus([e['text']]).input_ids[0]) < int(max_input_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YI6-pZdSqnI"
      },
      "source": [
        "# Limiting the size of the training dataset to simulate our low-data use case\n",
        "book_review_train_ds = book_review_ds[\"train\"].shuffle(seed=42).select(range(50))\n",
        "\n",
        "book_review_test_ds = book_review_ds[\"test\"] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIJPd8P9S9kW"
      },
      "source": [
        "## Data augmentation pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5mGMgKV20_w"
      },
      "source": [
        "### „äóÔ∏è Back-translation \n",
        "We'll be using the MariaMT model to perform back-translations, the translated sentences should be similar in context but not structurally identical. The back-translation process is as follows:\n",
        "\n",
        "1.   Translate a Dutch book review into French\n",
        "2.   Translate the resulting French text into English\n",
        "3.   Translate the resulting English text back into Dutch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTqRveiGRvdG"
      },
      "source": [
        "trans_pipeline_en_nl = pipeline(\n",
        "    task='translation_en_to_nl',\n",
        "    model='Helsinki-NLP/opus-mt-en-nl',\n",
        "    tokenizer='Helsinki-NLP/opus-mt-en-nl',\n",
        "    device=0)\n",
        "trans_pipeline_nl_fr = pipeline(\n",
        "    task='translation_nl_to_fr',\n",
        "    model='Helsinki-NLP/opus-mt-nl-fr',\n",
        "    tokenizer='Helsinki-NLP/opus-mt-nl-fr',\n",
        "    device=0)\n",
        "trans_pipeline_fr_en = pipeline(\n",
        "    task='translation_fr_to_en',\n",
        "    model='Helsinki-NLP/opus-mt-fr-en',\n",
        "    tokenizer='Helsinki-NLP/opus-mt-fr-en',\n",
        "    device=0)\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhN_1lVTSXWu"
      },
      "source": [
        "def back_tranlation_nl_fr_en_nl(texts):\n",
        "    fr_texts = trans_pipeline_nl_fr(texts)\n",
        "    back_translated_texts = trans_pipeline_fr_en([el['translation_text'] for el in fr_texts])\n",
        "    twohopback_translated_texts = trans_pipeline_en_nl([el['translation_text'] for el in back_translated_texts])\n",
        "    return [el['translation_text'] for el in twohopback_translated_texts]\n",
        "    \n",
        "backtranslate_dataset = lambda dataset: dataset.map(lambda x: {'text': back_tranlation_nl_fr_en_nl(x[\"text\"])}, batch_size=10, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gGKYuM0Sx05"
      },
      "source": [
        "# Back-translate the training dataset\n",
        "book_review_train_ds_back = backtranslate_dataset(book_review_train_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQIw3q-DSE9J"
      },
      "source": [
        "### ‚ú® Contextual word embedding insertions\n",
        "\n",
        "\n",
        "The [nlpaug](https://github.com/makcedward/nlpaug) library combines frequently used augmentation techniques into a python package. We'll use the `ContextualWordEmbsForSentenceAug` component that uses contextual word embeddings to find the top n similar words for augmentation. \n",
        "\n",
        "The contextual embeddings are retrieved from the tranformer-based pretrained RoBERTa model, which was trained on the Dutch section of the [OSCAR](https://oscar-corpus.com/) corpus. The word embeddings have a dependence on the surrounding words, this defines the **context** of the embededing.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mj-2mXiVKIR"
      },
      "source": [
        "aug = naf.Sequential([\n",
        "    naw.ContextualWordEmbsAug(\n",
        "        model_path='pdelobelle/robbert-v2-dutch-base',\n",
        "        model_type='roberta',\n",
        "        aug_p=0.20,\n",
        "        action=\"insert\")\n",
        "])\n",
        "\n",
        "replace_newline = lambda dataset: dataset.map(lambda x: {'text': x[\"text\"].replace(\"\\n\",' ')}, batched=False)\n",
        "contextual_emb_aug = lambda dataset: dataset.map(lambda x: {'text': aug.augment(x[\"text\"])},  batch_size=10, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2gfA_jeOStEN"
      },
      "source": [
        "# Removing newlines in the text and performing word insertions based on contextual word embeddings\n",
        "book_review_train_ds_newline = replace_newline(book_review_train_ds)\n",
        "book_review_train_ds_contemb = contextual_emb_aug(book_review_train_ds_newline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjKUMX0VSS_T"
      },
      "source": [
        "### üí™ Combination of both techniques\n",
        "Digging deeper into our bag of tricks üî•! \n",
        "\n",
        "This approach will combine both back-translation and contextual word embedding insertions as follows:\n",
        "\n",
        "1.   Inserting new words by using the contextual word-embeddings \n",
        "2.   Back-translate the augmented textual dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EDSDPa3-o9mT"
      },
      "source": [
        "# Combination of both contextual word embedding insertion and back-translation\n",
        "book_review_train_ds_contemb_back = backtranslate_dataset(book_review_train_ds_contemb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gesWuhM9jbLq"
      },
      "source": [
        "## üöÄ Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0xqcEiJqBZsB"
      },
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "epochs = 20\n",
        "max_steps = epochs * int(((len(book_review_train_ds)*3)/batch_size)) \n",
        "\n",
        "run_dicts = [] # list of dicts to store both metrics and logs for all the experiment runs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o-q6U8DAx0Jp"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "        Calculates the accuracy of the model's predictions, calculated as follows; (TP + TN) / (TP + TN + FP + FN) with TP: True positive TN: True negative FP: False positive FN: False negative\n",
        "    \"\"\"\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels) \n",
        "\n",
        "\n",
        "class LogAccumulatorCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A class that stores both the training and the evaluation loss\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.acc_logs = []\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        _ = logs.pop(\"total_flos\", None)\n",
        "        if state.is_local_process_zero and ('loss' in logs or 'eval_loss' in logs):\n",
        "            self.acc_logs.append(logs.copy())\n",
        "\n",
        "\n",
        "def train_and_evaluate(train_ds, test_ds, identifier):\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch['text'], padding=True, truncation=True)\n",
        "    \n",
        "    train_ds = train_ds.map(tokenize, batched=True, batch_size=len(train_ds), remove_columns=[\"text\"])\n",
        "    test_ds = test_ds.map(tokenize, batched=True, batch_size=len(test_ds), remove_columns=[\"text\"])\n",
        "    \n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        identifier,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        eval_steps=25,\n",
        "        logging_steps=25,\n",
        "        max_steps=max_steps,\n",
        "        learning_rate=2e-5,\n",
        "    )\n",
        "    \n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=2)\n",
        "\n",
        "    # Partially freezing the weights of initial layers of the model\n",
        "    # Since we're working on small datasets as it usually reduces overfitting\n",
        "    # Another advantage of partial freezing is reduced memory usage and a speed improvement during training.\n",
        "    for block in model.distilbert.embeddings.modules():\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad=False\n",
        "\n",
        "    for i in [0,1,2]:\n",
        "        for block in model.distilbert.transformer.layer[i].modules():\n",
        "            for param in block.parameters():\n",
        "                param.requires_grad=False\n",
        "\n",
        "            \n",
        "    logger = LogAccumulatorCallback()\n",
        "    trainer = Trainer(\n",
        "        model=model, args=training_args, \n",
        "        train_dataset=train_ds, \n",
        "        eval_dataset=test_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[logger],\n",
        "    )\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    \n",
        "    return metrics, logger.acc_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIp57un2x7Qy",
        "outputId": "9254321f-4eb6-4cca-88d8-db0b6776db97"
      },
      "source": [
        "### Model baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "108da803683d4b6285e31720dfb78f5a",
            "cceaff0558014568b82902544b066987"
          ]
        },
        "id": "5mDmH_lwU84r",
        "outputId": "763dce11-8257-4d56-b309-0b0cb2e5ba29"
      },
      "source": [
        "metrics, logs = train_and_evaluate(book_review_train_ds, book_review_test_ds, \"baseline\")\n",
        "\n",
        "run_dicts.append({\n",
        "    \"id\": \"baseline\",\n",
        "    \"metrics\": metrics,\n",
        "    \"logs\": logs\n",
        "})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "108da803683d4b6285e31720dfb78f5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cceaff0558014568b82902544b066987",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "***** Running training *****\n",
            "  Num examples = 50\n",
            "  Num Epochs = 52\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 360\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [360/360 01:01, Epoch 51/52]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.672600</td>\n",
              "      <td>0.699848</td>\n",
              "      <td>0.460292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.589300</td>\n",
              "      <td>0.669585</td>\n",
              "      <td>0.599676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.361900</td>\n",
              "      <td>0.732577</td>\n",
              "      <td>0.622366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.092600</td>\n",
              "      <td>0.973512</td>\n",
              "      <td>0.633712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.017800</td>\n",
              "      <td>1.224350</td>\n",
              "      <td>0.630470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.007400</td>\n",
              "      <td>1.358540</td>\n",
              "      <td>0.625608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>1.474566</td>\n",
              "      <td>0.633712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>1.477803</td>\n",
              "      <td>0.627229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>1.527897</td>\n",
              "      <td>0.630470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>1.577904</td>\n",
              "      <td>0.632091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.002400</td>\n",
              "      <td>1.604966</td>\n",
              "      <td>0.635332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>1.601137</td>\n",
              "      <td>0.633712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>1.609451</td>\n",
              "      <td>0.632091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>1.608002</td>\n",
              "      <td>0.632091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [78/78 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jge5A7tfo5Qp"
      },
      "source": [
        "\n",
        "### Model back-translated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d8881b633d1e4e79a0de8d27fc966606"
          ]
        },
        "id": "4zpt0ZyDU84s",
        "outputId": "d509b32b-8cc6-4f5d-97aa-a85897c4dbff"
      },
      "source": [
        "train_ds = concatenate_datasets([book_review_train_ds, book_review_train_ds_back])\n",
        "metrics, logs = train_and_evaluate(train_ds, book_review_test_ds, \"backtranslated\")\n",
        "\n",
        "run_dicts.append({\n",
        "    \"id\": \"backtranslated\",\n",
        "    \"metrics\": metrics,\n",
        "    \"logs\": logs\n",
        "})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8881b633d1e4e79a0de8d27fc966606",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/dbrd/plain_text/3.0.0/a454f53ccf247517cbb44e57f07904d4adefc5837d766f6120ff467ea7a465f7/cache-721aee2d2ce41938.arrow\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.8.0\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 28\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 360\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [360/360 01:02, Epoch 27/28]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.699700</td>\n",
              "      <td>0.697789</td>\n",
              "      <td>0.444084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.636800</td>\n",
              "      <td>0.652168</td>\n",
              "      <td>0.619125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.457200</td>\n",
              "      <td>0.635084</td>\n",
              "      <td>0.646677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.222400</td>\n",
              "      <td>0.806585</td>\n",
              "      <td>0.654781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.064900</td>\n",
              "      <td>1.133113</td>\n",
              "      <td>0.653160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>1.307500</td>\n",
              "      <td>0.666126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>1.473922</td>\n",
              "      <td>0.654781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.003900</td>\n",
              "      <td>1.515776</td>\n",
              "      <td>0.659643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>1.551090</td>\n",
              "      <td>0.661264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>1.628136</td>\n",
              "      <td>0.649919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>1.757906</td>\n",
              "      <td>0.641815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>1.741461</td>\n",
              "      <td>0.646677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>1.721531</td>\n",
              "      <td>0.641815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>1.722520</td>\n",
              "      <td>0.645057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [78/78 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoVKrPvMU84s"
      },
      "source": [
        "### Model contextual word embedding insertions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "a3315ab44057410fb5cc3aebd23e029e"
          ]
        },
        "id": "wurkCEMOU84s",
        "outputId": "e7cb16e4-66a4-4b91-d91a-77e338d757df"
      },
      "source": [
        "train_ds = concatenate_datasets([book_review_train_ds, book_review_train_ds_contemb])\n",
        "\n",
        "metrics, logs = train_and_evaluate(train_ds, book_review_test_ds, \"contextual_embedding\")\n",
        "\n",
        "run_dicts.append({\n",
        "    \"id\": \"contextual_embedding\",\n",
        "    \"metrics\": metrics,\n",
        "    \"logs\": logs\n",
        "})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3315ab44057410fb5cc3aebd23e029e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/dbrd/plain_text/3.0.0/a454f53ccf247517cbb44e57f07904d4adefc5837d766f6120ff467ea7a465f7/cache-721aee2d2ce41938.arrow\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.8.0\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 28\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 360\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [360/360 01:06, Epoch 27/28]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.685700</td>\n",
              "      <td>0.688230</td>\n",
              "      <td>0.544571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.603500</td>\n",
              "      <td>0.646854</td>\n",
              "      <td>0.614263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.360300</td>\n",
              "      <td>0.685971</td>\n",
              "      <td>0.638574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.132300</td>\n",
              "      <td>0.871569</td>\n",
              "      <td>0.662885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.027900</td>\n",
              "      <td>1.138665</td>\n",
              "      <td>0.666126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.007900</td>\n",
              "      <td>1.330548</td>\n",
              "      <td>0.656402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>1.401971</td>\n",
              "      <td>0.659643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>1.439451</td>\n",
              "      <td>0.662885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>1.459663</td>\n",
              "      <td>0.666126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>1.517904</td>\n",
              "      <td>0.659643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>1.535877</td>\n",
              "      <td>0.664506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>1.540283</td>\n",
              "      <td>0.659643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>1.549505</td>\n",
              "      <td>0.662885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>1.553675</td>\n",
              "      <td>0.664506</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [78/78 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxt1VqkmU84t"
      },
      "source": [
        "### Model back-translated & contextual word embedding insertions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0b0476bc97df4d0da11546a0a41c6eac"
          ]
        },
        "id": "PVYDDCjkU84t",
        "outputId": "88bafe75-9526-424b-dfe5-e170bd4ade47"
      },
      "source": [
        "train_ds = concatenate_datasets([book_review_train_ds,  book_review_train_ds_contemb_back])\n",
        "\n",
        "metrics, logs = train_and_evaluate(train_ds, book_review_test_ds, \"backtranslated_contextual_embedding\")\n",
        "\n",
        "run_dicts.append({\n",
        "    \"id\": \"backtranslated_contextual_embedding\",\n",
        "    \"metrics\": metrics,\n",
        "    \"logs\": logs\n",
        "})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b0476bc97df4d0da11546a0a41c6eac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/dbrd/plain_text/3.0.0/a454f53ccf247517cbb44e57f07904d4adefc5837d766f6120ff467ea7a465f7/cache-721aee2d2ce41938.arrow\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.8.0\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 28\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 360\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [360/360 01:04, Epoch 27/28]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.696100</td>\n",
              "      <td>0.699685</td>\n",
              "      <td>0.444084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.657100</td>\n",
              "      <td>0.674882</td>\n",
              "      <td>0.583468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.526000</td>\n",
              "      <td>0.629287</td>\n",
              "      <td>0.619125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.276000</td>\n",
              "      <td>0.649577</td>\n",
              "      <td>0.675851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.102600</td>\n",
              "      <td>0.786472</td>\n",
              "      <td>0.688817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.021500</td>\n",
              "      <td>0.960320</td>\n",
              "      <td>0.703404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.009600</td>\n",
              "      <td>1.085739</td>\n",
              "      <td>0.695300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.005100</td>\n",
              "      <td>1.151869</td>\n",
              "      <td>0.695300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.003900</td>\n",
              "      <td>1.220545</td>\n",
              "      <td>0.692058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>1.229985</td>\n",
              "      <td>0.690438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>1.248001</td>\n",
              "      <td>0.695300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.002600</td>\n",
              "      <td>1.251380</td>\n",
              "      <td>0.700162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.002400</td>\n",
              "      <td>1.267554</td>\n",
              "      <td>0.696921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>1.277560</td>\n",
              "      <td>0.695300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 617\n",
            "  Batch size = 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [78/78 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoGbjDeBvL9t"
      },
      "source": [
        "### Model with combined augmented datasets, the whole merry gang together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swyOEiqdvZwq"
      },
      "source": [
        "train_ds = concatenate_datasets([book_review_train_ds, book_review_train_ds_back, book_review_train_ds_contemb, book_review_train_ds_contemb_back])\n",
        "\n",
        "metrics, logs = train_and_evaluate(train_ds, book_review_test_ds, \"combined_augmented_data\")\n",
        "\n",
        "run_dicts.append({\n",
        "    \"id\": \"combined_augmented_data\",\n",
        "    \"metrics\": metrics,\n",
        "    \"logs\": logs\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TKTBiKUU84t"
      },
      "source": [
        "##  üìä Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3itfEYbTU84t"
      },
      "source": [
        "df = pd.DataFrame(run_dicts)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl7lOBjjTjGK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "06390c5f-b746-4b3d-91cf-d2441fa2c10c"
      },
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    \n",
        "    fig.add_trace(go.Scatter(\n",
        "                    x=list(range(25,max_steps,25)),\n",
        "                    y=pd.DataFrame(row['logs']).dropna(subset=['eval_accuracy'])['eval_accuracy'],\n",
        "                    name='accuracy {}'.format(row['id'])))\n",
        "\n",
        "fig.update_xaxes(title_text='step')\n",
        "fig.update_yaxes(title_text='accuracy')\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"bb2192de-9b6d-4aae-a376-9d52664d4cfe\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"bb2192de-9b6d-4aae-a376-9d52664d4cfe\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'bb2192de-9b6d-4aae-a376-9d52664d4cfe',\n",
              "                        [{\"name\": \"accuracy baseline\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350], \"y\": [0.46029173419773095, 0.5996758508914101, 0.6223662884927067, 0.6337115072933549, 0.6304700162074555, 0.6256077795786061, 0.6337115072933549, 0.6272285251215559, 0.6304700162074555, 0.6320907617504052, 0.6353322528363047, 0.6337115072933549, 0.6320907617504052, 0.6320907617504052, 0.6320907617504052]}, {\"name\": \"accuracy backtranslated\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350], \"y\": [0.44408427876823336, 0.6191247974068071, 0.646677471636953, 0.6547811993517018, 0.653160453808752, 0.6661264181523501, 0.6547811993517018, 0.6596434359805511, 0.6612641815235009, 0.6499189627228525, 0.6418152350081038, 0.646677471636953, 0.6418152350081038, 0.6450567260940032, 0.6434359805510534]}, {\"name\": \"accuracy contextual_embedding\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350], \"y\": [0.5445705024311183, 0.6142625607779578, 0.6385737439222042, 0.6628849270664505, 0.6661264181523501, 0.6564019448946515, 0.6596434359805511, 0.6628849270664505, 0.6661264181523501, 0.6596434359805511, 0.6645056726094003, 0.6596434359805511, 0.6628849270664505, 0.6645056726094003, 0.6645056726094003]}, {\"name\": \"accuracy backtranslated_contextual_embedding\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350], \"y\": [0.44408427876823336, 0.5834683954619124, 0.6191247974068071, 0.6758508914100486, 0.6888168557536467, 0.7034035656401945, 0.6952998379254457, 0.6952998379254457, 0.6920583468395461, 0.6904376012965965, 0.6952998379254457, 0.700162074554295, 0.6969205834683955, 0.6952998379254457, 0.6936790923824959]}, {\"name\": \"accuracy combined_augmented_data\", \"type\": \"scatter\", \"x\": [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350], \"y\": [0.44408427876823336, 0.6077795786061588, 0.6320907617504052, 0.6645056726094003, 0.6904376012965965, 0.6888168557536467, 0.6969205834683955, 0.6855753646677472, 0.6920583468395461, 0.6839546191247974, 0.6839546191247974, 0.6855753646677472, 0.6888168557536467, 0.6839546191247974, 0.6839546191247974]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"title\": {\"text\": \"step\"}}, \"yaxis\": {\"title\": {\"text\": \"accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bb2192de-9b6d-4aae-a376-9d52664d4cfe');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ALJ0WldXtrp"
      },
      "source": [
        "## üèÅ Take-aways \n",
        "\n",
        "\n",
        "You've reached the finish line! üëè  Let's sum up some of the findings.\n",
        "\n",
        "* Both back-translation and contextual word embedding insertions boosted the robustness and performance of the model üëå \n",
        "* Creativity also helps! üé® The combination of both back-translation and contextual word embedding insertions achieved the highest performance. \n",
        "* The goal is to use context-preserving augmentation techniques that generate structurally different sentences while preserving the meaning.\n",
        "* The data from the DBRD dataset was well-represented by the pretrained model, such that training without data-augmentation techniques already yielded good results\n",
        "\n",
        "We considered 3-hop backtranslation between Dutch, French and English, but you could also include other languages and more hops to generate even more samples . \n",
        "\n",
        "You could also try out other text augmentation techniques such as: Synonym Replacement, Random Insertion, Random Swap, Random Deletion. üïµÔ∏è‚Äç‚ôÇÔ∏è\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}