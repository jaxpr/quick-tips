# GPT2 Quantization using ONNXRuntime

This notebook accompanies our blogpost on using ONNXRuntime to quantize our Dutch GPT2 model to reduce costs and latency in [our application](https://gpt2.ml6.eu).


We recommend to open the notebook using Colab for an interactive explainable experience and optimal rendering of the visuals ðŸ‘‡:


[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ml6team/quick-tips/blob/main/nlp/gpt2_quantization_onnxruntime/gpt2_quantization.ipynb)

