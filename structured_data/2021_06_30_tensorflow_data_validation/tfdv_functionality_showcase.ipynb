{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfdv_cc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2nL1hsS2vGF"
      },
      "source": [
        "# TensorFlow Data Validation\n",
        "This notebook goes over the different functionalities of standalone TensorFlow Data Validation (TFDV). This was covered in the 2021 Q2 chapter conference. [Here is the link to the video](https://drive.google.com/file/d/12YUv_k-ORXjcJ_pqGEXHCIJvf8lsw7Sy) if you want to find out more about data drift & skew, and divergence metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX1etcuZGv9f"
      },
      "source": [
        "# Perform the necessary installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr5wJkaVGaJi"
      },
      "source": [
        "!pip install tensorflow_data_validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siZyucxiZN3s"
      },
      "source": [
        "Restart runtime before continuing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzsyC71AHtF5"
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow_data_validation as tfdv\n",
        "from tensorflow_data_validation.utils.schema_util import schema_pb2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9fk0OfTZWlv"
      },
      "source": [
        "Download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr2eszfaxs6R"
      },
      "source": [
        "zip, headers = urllib.request.urlretrieve('https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip')\n",
        "zipfile.ZipFile(zip).extractall()\n",
        "zipfile.ZipFile(zip).close()\n",
        "\n",
        "train_data = pd.read_csv(os.path.join('.', 'data', 'train', 'data.csv'))\n",
        "test_data = pd.read_csv(os.path.join('data', 'eval', 'data.csv'))\n",
        "serving_data = pd.read_csv(os.path.join('data', 'serving', 'data.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZjGwAXXosOY"
      },
      "source": [
        "# Generating and visualizing statistics\n",
        "You can generate statistics from a dataframe, tfrecord, or a CSV file.\n",
        "\n",
        "We can generate and visualize statistics of a dataset as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcviD6xpFUZo"
      },
      "source": [
        "train_stats = tfdv.generate_statistics_from_dataframe(train_data)\n",
        "tfdv.visualize_statistics(train_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "222a_I8SLMQ5"
      },
      "source": [
        "Some useful features on this chart are the log checkmark and the possbility to view quantiles (when clicking the dropdown on the top right).\n",
        "\n",
        "Some of the possible issues that are flagged:\n",
        "* `pickup_cencus_tract` is always null\n",
        "* `dropoff_census_tract` & `company` are missing quire a lot\n",
        "* Lots of zero values for `trip_miles`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqoFnakzSDv4"
      },
      "source": [
        "# Schema inference\n",
        "Using the generated statistics, a data schema can be generated and shown as is done below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qoh-RSnhLL_o"
      },
      "source": [
        "schema = tfdv.infer_schema(statistics=train_stats)\n",
        "tfdv.display_schema(schema=schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeXNJ0dhNuBA"
      },
      "source": [
        "The `presence` column indicates if a feature can be missing. The `valency` column indicates the number of values that are required for that feature per training sample (for categorical features, single implies there must be exactly one category per sample).\n",
        "\n",
        "This automatically inferred schema can be a good starting point for a custom schema. You can access the features of a schema using `tfdv.get_feature(schema, 'feature_name')`. You can access the domain of a feature using `tfdv.get_domain(schema, 'feature_name')`, or set it using `tfdv.set_domain(schema, 'feature_name', domain)`.\n",
        "\n",
        "Suppose we want to set the `trip_miles` feature to be between 0 and 500, based on the statistics of the training data we saw before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBMC06awN9fJ"
      },
      "source": [
        "tfdv.set_domain(schema, 'trip_miles', schema_pb2.FloatDomain(min=0, max=500))\n",
        "\n",
        "tfdv.display_schema(schema=schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgsULEpbSmxT"
      },
      "source": [
        "# Comparing two datasets\n",
        "Now that we have our schema, let's check out the test set. We can visualize them next to each other by specifying a right hand side and left hand side in the `visualize_statistics` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXXN7nvOFdVh"
      },
      "source": [
        "test_stats = tfdv.generate_statistics_from_dataframe(test_data)\n",
        "tfdv.visualize_statistics(lhs_statistics=test_stats, rhs_statistics=train_stats, lhs_name='test dataset', rhs_name='train dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyk0qqvrWFbr"
      },
      "source": [
        "Some issues can be spotted by looking at the differences in minimum and max values. Also, note the percentages checkbox that appears when comparing two datasets. This allows for easier comparison of the distributions of the datasets.\n",
        "\n",
        "We can check for schema anomalies using the `validate_statistics` method. This checks if the input statistics are conform to the earlier defined schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLCJwc_nFuH9"
      },
      "source": [
        "anomalies = tfdv.validate_statistics(statistics=test_stats, schema=schema)\n",
        "tfdv.display_anomalies(anomalies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfspkTPlXwjL"
      },
      "source": [
        "There are unexpected values for the categorical field `company`, but all of which only occur less than 1% of the time. `payment_type` also has an unexpected field. `trip_miles` does not conform to the domain we set earlier.\n",
        "\n",
        "You can access the feature of a schema using `tfdv.get_feature(schema, 'feature_name)`.\n",
        "You can access the domain of a feature using `tfdv.get_domain(schema, 'feature_name')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B91hn0RMGSKm"
      },
      "source": [
        "# Relax requirement on unexpected string values for company categories\n",
        "company = tfdv.get_feature(schema, 'company')\n",
        "company.distribution_constraints.min_domain_mass = 0.9\n",
        "\n",
        "# Add a new category to the payment type domain\n",
        "payment_type_domain = tfdv.get_domain(schema, 'payment_type')\n",
        "payment_type_domain.value.append('Prcard')\n",
        "\n",
        "# Increase domain range of trip_miles\n",
        "trip_miles_domain = tfdv.get_domain(schema, 'trip_miles')\n",
        "trip_miles_domain.max = 2000.0\n",
        "\n",
        "updated_anomalies = tfdv.validate_statistics(test_stats, schema)\n",
        "tfdv.display_anomalies(updated_anomalies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC6l4Yqe8rcb"
      },
      "source": [
        "Other schema changes you can do include:\n",
        "* Changing the type of the feature: `feature.type = 1` (`0` for unknown, `1` for string, `2` for int, `3` for float, `4` for struct)\n",
        "* Setting the minimum required presence of a feature: `feature.presence.min_fraction=0.9`\n",
        "* Changing the valency of a feature: `feature.value_count.min = min` or `feature.value_count.max = max`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djwii2jZVpJL"
      },
      "source": [
        "# Environment-based schema\n",
        "\n",
        "The schema can differ per environment. For example: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blVQeHjhTVbw"
      },
      "source": [
        "serving_stats = tfdv.generate_statistics_from_dataframe(serving_data)\n",
        "serving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n",
        "\n",
        "tfdv.display_anomalies(serving_anomalies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QjBUkws9Tj4"
      },
      "source": [
        "Here we see that the tips value is missing from the serving data. This is due to the fact that this is our label value, which is absent from serving data.\n",
        "\n",
        "This is were environments come in. You can create extra environments for your schema using `schema.default_environment.append('env_name')` and add or remove a feature from this using `feature.in_environment.append('env_name')` or `feature.not_in_environment.append('env_name')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZlNWX3ISxVP"
      },
      "source": [
        "schema.default_environment.append('TRAINING')\n",
        "schema.default_environment.append('SERVING')\n",
        "\n",
        "tips_feature = tfdv.get_feature(schema, 'tips')\n",
        "tips_feature.not_in_environment.append('SERVING')\n",
        "\n",
        "serving_anomalies = tfdv.validate_statistics(serving_stats, schema, environment='SERVING')\n",
        "tfdv.display_anomalies(serving_anomalies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71DAstmGbicI"
      },
      "source": [
        "# Distribution Drift & Skew"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8upbZbX7gOU6"
      },
      "source": [
        "Using the skew & drift comparator, you can configure your schema to allow detection of drift and skew. Both comparators are essentially the same. The skew comparator mean to be used to detect changes between training and serving statistics, and the drift comparator is used to detect changes between two different datasets in time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SryLKWCTe7o"
      },
      "source": [
        "pickup_community_area_feature = tfdv.get_feature(schema, 'pickup_community_area')\n",
        "pickup_community_area_feature.drift_comparator.jensen_shannon_divergence.threshold = 0.2\n",
        "pickup_community_area_feature.skew_comparator.jensen_shannon_divergence.threshold = 0.2\n",
        "\n",
        "company_feature=tfdv.get_feature(schema, 'company')\n",
        "company_feature.drift_comparator.infinity_norm.threshold = 0.001\n",
        "company_feature.skew_comparator.infinity_norm.threshold = 0.2\n",
        "\n",
        "payment_type_feature = tfdv.get_feature(schema, 'payment_type')\n",
        "payment_type_feature.drift_comparator.infinity_norm.threshold = 0.001\n",
        "payment_type_feature.skew_comparator.infinity_norm.threshold = 0.001\n",
        "\n",
        "skew_anomalies = tfdv.validate_statistics(statistics=train_stats, schema=schema,\n",
        "                                          serving_statistics=serving_stats,\n",
        "                                          previous_statistics=test_stats)\n",
        "tfdv.display_anomalies(skew_anomalies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6jM9xypWyLI"
      },
      "source": [
        "# Saving your schema\n",
        "\n",
        "Once we are happy with our custom schema, we can export it so it can be loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayXhol_VW90K"
      },
      "source": [
        "tfdv.write_schema_text(schema, 'schema.txt')\n",
        "loaded_schema = tfdv.load_schema_text('schema.txt')\n",
        "tfdv.display_schema(loaded_schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLr0hwCVXG9Z"
      },
      "source": [
        "We can also choose to write out the anomalies and statistics to a file if we want to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vArV6zurXLqd"
      },
      "source": [
        "tfdv.write_anomalies_text(skew_anomalies, 'anomalies.txt')\n",
        "tfdv.write_stats_text(train_stats, 'train_stats.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efzERJRkbSyw"
      },
      "source": [
        "# Slicing your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMnt7r-siKXR"
      },
      "source": [
        "Another functinality of tfdv is the ability to slice your data. This can be useful when you want to analyze the distribution of certain categorical values.\n",
        "\n",
        "In the example below, we slice the data to look at the distribution of occurrences where no payment was charged.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_DhInVebULM"
      },
      "source": [
        "from tensorflow_data_validation.utils import slicing_util\n",
        "from tensorflow_data_validation.utils.stats_util import statistics_pb2\n",
        "\n",
        "slice_cash_function = slicing_util.get_feature_value_slicer(features={'payment_type': [b'No Charge']})\n",
        "slice_options = tfdv.StatsOptions(slice_functions=[slice_cash_function])\n",
        "# This functionality does not seem to work for dataframes\n",
        "slice_stats = tfdv.generate_statistics_from_csv('data/train/data.csv', stats_options=slice_options)\n",
        "\n",
        "def get_sliced_stats(stats, slice_key):\n",
        "    for sliced_stats in stats.datasets:\n",
        "        if sliced_stats.name == slice_key:\n",
        "            result = statistics_pb2.DatasetFeatureStatisticsList()\n",
        "            result.datasets.add().CopyFrom(sliced_stats)\n",
        "            return result\n",
        "        print('Invalid Slice key')\n",
        "\n",
        "def display_slice_keys(stats):\n",
        "    print(list(map(lambda x: x.name, slice_stats.datasets)))\n",
        "\n",
        "display_slice_keys(slice_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKuHPgddb4ZO"
      },
      "source": [
        "tfdv.visualize_statistics(get_sliced_stats(slice_stats, 'All Examples'), get_sliced_stats(slice_stats, 'payment_type_No Charge'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}